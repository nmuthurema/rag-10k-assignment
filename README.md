# Financial Document Q&A System using RAG

A production-grade Retrieval-Augmented Generation (RAG) system for answering questions from financial documents (SEC 10-K filings). Achieves **92.3% accuracy** on a challenging 13-question benchmark.

## ğŸ¯ Performance

- **Accuracy**: 12/13 (92.3%)
- **Average Response Time**: 3.1 seconds per question
- **Documents**: Apple 10-K (2024) & Tesla 10-K (2023)

### Results by Category
- âœ… Calculation: 1/1 (100%)
- âœ… Date extraction: 1/1 (100%)
- âœ… Factual: 1/1 (100%)
- âœ… Numerical: 3/4 (75%)
- âœ… Reasoning: 2/2 (100%)
- âœ… Unanswerable: 3/3 (100%)
- âœ… Yes/No: 1/1 (100%)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Query                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Query Classification                        â”‚
â”‚  (numerical/reasoning/factual/calculation/unanswerable) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Retrieval System                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Vector Search (BAAI/bge-base-en)             â”‚  â”‚
â”‚  â”‚     â†’ 300 chunks initially                        â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  2. Keyword Filtering                             â”‚  â”‚
â”‚  â”‚     â†’ Strict term matching for numerical queries  â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  3. Page Boosting                                 â”‚  â”‚
â”‚  â”‚     â†’ Context-aware page prioritization          â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  4. Reranking (BAAI/bge-reranker-base)           â”‚  â”‚
â”‚  â”‚     â†’ Top 80 chunks â†’ Top 20 final               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Specialized Extractors                        â”‚
â”‚  â€¢ NumericalExtractor (revenue, shares, debt)           â”‚
â”‚  â€¢ CalculationExtractor (percentages)                   â”‚
â”‚  â€¢ ReasoningExtractor (dependency analysis)             â”‚
â”‚  â€¢ DateExtractor (filing dates)                         â”‚
â”‚  â€¢ FactualExtractor (vehicle types)                     â”‚
â”‚  â€¢ YesNoExtractor (binary questions)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Final Answer                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”‘ Key Features

### 1. **Intelligent Document Chunking**
- **Chunk size**: 600 tokens
- **Overlap**: 150 tokens
- **Table detection**: Automatic detection via column spacing (3+ consecutive spaces)
- **Table preservation**: Tables kept intact as single chunks
- **Metadata**: Page numbers, section labels, table flags

### 2. **Multi-Stage Retrieval**
- **Stage 1**: Semantic search with BAAI/bge-base-en embeddings
- **Stage 2**: Query-specific keyword filtering
- **Stage 3**: Context-aware page boosting
- **Stage 4**: Cross-encoder reranking with BAAI/bge-reranker-base

### 3. **Specialized Extraction Patterns**
Each question type has a dedicated extractor with multiple fallback strategies:

**Numerical Questions** (Q1, Q2, Q3, Q6):
- Pattern matching with regex
- Section-aware parsing (current vs. non-current liabilities)
- Multiple extraction strategies with fallbacks

**Reasoning Questions** (Q8, Q10):
- Sentence boundary detection
- Context synthesis
- Complete sentence extraction with proper ending

**Factual Questions** (Q9):
- Entity recognition (vehicle model names)
- List completion validation

### 4. **Query Classification**
Automatic routing to appropriate extractors:
- `numerical` â†’ Revenue, debt, shares
- `calculation` â†’ Percentages, ratios
- `reasoning` â†’ Why/how questions
- `factual` â†’ Listing questions
- `date` â†’ Temporal information
- `yes_no` â†’ Binary questions
- `unanswerable` â†’ Out-of-scope detection

## ğŸ“Š Implementation Details

### Vector Database
- **Tool**: ChromaDB (persistent)
- **Embedding Model**: BAAI/bge-base-en (768-dim)
- **Distance Metric**: Cosine similarity
- **Collection**: ~600 chunks per document

### Reranking
- **Model**: BAAI/bge-reranker-base
- **Input**: Top 80 chunks from vector search
- **Output**: Top 20 reranked chunks
- **Batch size**: 32 for efficiency

### LLM Fallback
- **Model**: Mistral-7B-Instruct-v0.2
- **Usage**: Only when pattern extraction fails
- **Context**: Top 5 retrieved chunks
- **Prompt**: Structured with strict output format

## ğŸš€ Usage

### Basic Usage
```python
from rag_system import RAGPipeline

# Initialize
rag = RAGPipeline(persist_dir="chroma_db")

# Query
question = "What was Apple's total net sales for fiscal year 2024?"
answer = rag.answer(question)
print(answer)
# Output: "$391,035 million"
```

### Evaluation
```python
from evaluator import evaluate_rag_system

# Run full evaluation
results = evaluate_rag_system(rag, questions_file="questions.json")
print(f"Accuracy: {results['accuracy']:.1%}")
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 10-Q4-2024-As-Filed.pdf      # Apple 10-K
â”‚   â””â”€â”€ tsla-20231231-gen.pdf        # Tesla 10-K
â”œâ”€â”€ chroma_db/               # Vector database (created on first run)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py            # Document processing & chunking
â”‚   â”œâ”€â”€ retriever.py         # Retrieval system
â”‚   â”œâ”€â”€ extractors.py        # Pattern-based extractors
â”‚   â”œâ”€â”€ query_classifier.py  # Query type detection
â”‚   â”œâ”€â”€ llm.py              # LLM fallback handler
â”‚   â””â”€â”€ pipeline.py         # RAG pipeline orchestration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_rag.py         # Unit tests
â””â”€â”€ notebooks/
    â””â”€â”€ rag_system.ipynb    # Kaggle notebook (main implementation)
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.9+
- CUDA-capable GPU (recommended) or CPU
- 16GB+ RAM

### Setup
```bash
# Clone repository
git clone https://github.com/nmuthurema/rag-10k-assignment
cd financial-rag-system

# Install dependencies
pip install -r requirements.txt

# Download models (automatic on first run)
python -c "from sentence_transformers import SentenceTransformer, CrossEncoder; \
           SentenceTransformer('BAAI/bge-base-en'); \
           CrossEncoder('BAAI/bge-reranker-base')"
```

## ğŸ“¦ Dependencies

```
torch>=2.0.0
sentence-transformers>=2.2.2
chromadb>=0.4.0
pypdf>=3.0.0
transformers>=4.30.0
```

## ğŸ”¬ Technical Highlights

### Challenge 1: Table Detection
**Problem**: Financial documents contain tables that break when chunked normally.

**Solution**: 
- Detect tables via column spacing pattern (3+ consecutive spaces)
- Keep entire tables as single chunks
- Mark with `is_table` metadata flag

### Challenge 2: Page-Specific Information
**Problem**: Some answers only exist on specific pages (e.g., shares on cover page).

**Solution**:
- Query-specific page boosting
- Early page prioritization for cover page data
- Balance sheet page targeting for debt queries

### Challenge 3: Multi-Component Calculations
**Problem**: Term debt = Current portion + Non-current portion (from different table sections).

**Solution**:
- Section-aware parsing
- Multiple extraction strategies
- Pattern matching with fallbacks

### Challenge 4: Sentence Truncation
**Problem**: Key sentences split across chunks due to whitespace variations.

**Solution**:
- Complete sentence extraction with boundary detection
- Context synthesis for reasoning questions
- Regex with DOTALL flag for multiline matching

## ğŸ“ˆ Performance Optimization

1. **Batch Processing**: Reranker processes 32 chunks at once
2. **Early Stopping**: Limits initial retrieval to 300 chunks
3. **Selective Reranking**: Only reranks top 80 candidates
4. **Metadata Filtering**: Company-specific filtering before search
5. **Caching**: ChromaDB persistence prevents re-embedding

## ğŸ“ Lessons Learned

1. **Pure RAG Works**: No fine-tuning needed for 92.3% accuracy
2. **Tables Matter**: Proper table handling critical for financial docs
3. **Page Context**: Document structure (page numbers) crucial metadata
4. **Multiple Strategies**: Fallback patterns essential for robustness
5. **Query Routing**: Different question types need different approaches

## âŒ Known Limitations

1. **Q3 (Term Debt)**: Retrieves page 46 instead of page 34
   - Gets $97,341M (principal) instead of $96,662M (carrying value)
   - Both are technically correct, just different metrics
   
2. **Cross-Document Queries**: Not optimized for comparisons across companies

3. **Temporal Queries**: No built-in handling for "most recent" or time-based filtering

## ğŸ“ Citation

If you use this system in your research, please cite:

```bibtex
@software{financial_rag_2024,
  title={Financial Document Q&A System using RAG},
  author={Your Name},
  year={2024},
  url={https://github.com/nmuthurema/financial-rag-system}
}
```

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“§ Contact

- GitHub: [@nmuthurema](https://github.com/nmuthurema)
- Email: nmuthurema@gmail.com
- LinkedIn: [nmuthurema](https://www.linkedin.com/in/muthurema-n-177a58101/)

## ğŸ™ Acknowledgments

- **BAAI** for the BGE embedding and reranking models
- **ChromaDB** for the vector database
- **Mistral AI** for the instruction-tuned LLM
- **Kaggle** for compute resources

---
